### 项目地址

​	 https://github.com/udacity/Linear-Algebra-cn 

### 项目反馈1

作为第一次提交的项目，你取得了一个非常不错的成绩，继续加油，具体问题请参考下面的审阅内容：

### 第一部分

1. 掌握向量、矩阵的基本运算规则以及性质，并且能够利用numpy进行求解和证明。

因为审阅框架的问题，批改顺序有些不太一样，望见谅~
1.2至3的证明题中：
习题1.2.1和习题1.2.2 ，需要修改

[![1.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1546618454/1.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1546618454/1.png)
[![2.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1546618455/2.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1546618455/2.png)

1. 正确的绘制出了向量的加法运算，能够利用绘图验证了线性方程组解的唯一性，并且进行了方程组的求解。

习题1.1.1 计算正确，需要补充绘制
需要实现类似这个效果的图
[![1.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1534139360/1.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1534139360/1.png)

1. 掌握向量范数的计算，成功实现了编程求解，并且计算函数考虑到了边界条件，通过了单元测试。

习题4.1，正确计算了向量的范数！

------

下图给出了一个Lp球的形状随着P的减少的可视化图
[![范数.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1534137497/%E8%8C%83%E6%95%B0.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1534137497/范数.png)
下面来讲讲范数在机器学习中的应用
首先L0表示向量中所有非零元素的个数。正是L0范数的这个属性，使得其非常适合机器学习中稀疏编码，特征选择的应用。通过最小化L0范数，来寻找最少最优的稀疏特征项。
再来是L1范数，其应用范围非常的广泛。如在计算机视觉中的[Sum of Absolute Differents](https://en.wikipedia.org/wiki/Sum_of_absolute_differences)，[Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error)，还有我们[Lasso回归](https://en.wikipedia.org/wiki/Least_squares#Lasso_method)都是利用L1范式。
最后是L2范数，L2范数是最常见，也最著名的范数，我们常见的[Ridge回归](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf)，就是运用L2范数作为惩罚项推导出来的。

------

可能上面的扩展你会觉得很慌哈，没有关系，在后面机器学习进阶中，你会有更深的认识，这里先留个总体概念即可。当下可以[参考这里来进行初步的学习](https://www.zhihu.com/question/20473040)。

1. 掌握矩阵的迹与范数的基本原理与联系，成功实现了编程求解，并且计算函数考虑到了边界条件，通过了单元测试。

习题4.2 正确计算了矩阵的范数~

### 第二部分

1. 实现了矩阵条件数的计算，理解矩阵条件数以及线性系统之间的稳定性关系。

习题4.3 成功计算出了矩阵的条件数！

1. 进一步深入理解矩阵条件数的应用，能够理解Ridge Regression与L2范数、条件数之间的联系。

选做不做算对，虽然没做，但这里也给你一些解释，希望后面继续学习ai的时候你回过头来发现这里有宝藏，哈哈
condition number 是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的 condition number 很小，那么它就是well-conditioned的，如果非常大，那么它就是 ill-conditioned 的，如果一个系统是 ill-conditioned 的，它的输出结果就不要太相信了。

这里再解释下ill-conditioned，这种病态条件的线性系统，或者说这种不够稳定的线性系统我们有个名词，中文叫病态系统，那么线性系统 Ax = b 为什么会病态（不稳定）？
归根到底是由于 A 矩阵列向量线性相关性过大，表示的特征太过于相似以至于容易混淆所产生的。举个例子, 现有一个两个十分相似的列向量组成的矩阵 A：
[![病态系统.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1534141526/%E7%97%85%E6%80%81%E7%B3%BB%E7%BB%9F.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1534141526/病态系统.png)
在二维空间上，这两个列向量夹角非常小。假设第一次检测得到数据 b = [1000, 0]^T, 这个点正好在第一个列向量所在的直线上，解集是 [1, 0]^T。现在再次检测，由于有轻微的误差，得到的检测数据是 b = [1000, 0.001]， 这个点正好在第二个列向量所在的直线上，解集是 [0, 1]^T。两次求得到了差别迥异的的解集。
（所以照上面的举例来看，是不是可以看出单位矩阵是一个非常稳定的矩阵呢）

1. 掌握奇异值分解的原理，能够理解奇异值分解最常见的两个应用，降维以及矩阵重构，成功实现了编程求解，通过了单元测试。

习题5.1与习题5.5非常好
完全正确

1. 整体上编程实现逻辑严谨、代码规范，证明与问答部分合理且规范，绘图部分清晰规整。

1. 计算出了不同降维大小下重构矩阵的Frobenius范数损失，并且利用绘图方式进一步理解奇异值分解以及其应用。

选做不做算对

------

[![5_3.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1540113201/5_3.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1540113201/5_3.png)

[![5_2.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1540113201/5_2.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1540113201/5_2.png)

[![5_4.png](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1540113201/5_4.png)](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/232452/1540113201/5_4.png)
上面是几道选做的答案

------

下面来对这三道选做题进行一些解释：
对于奇异值,在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：
ps:下划线后的数据代表小标，指几乘几矩阵
A_m×n=(U_m×m)(Σ_m×n)(VT_n×n)≈(U_m×k)(Σ_k×k)(VT_k×n)
其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵U_m×k,Σ_k×k,VT_k×n来表示,由于这个重要的性质，SVD可以用于[PCA](https://zh.wikipedia.org/zh-hans/主成分分析)降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于[NLP](https://baike.baidu.com/item/NLP/25220)中的算法，比如潜在语义索引（LSI）。

 重新提交项目


 

